

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>From RAG to Context - A 2025 Year-End Review of RAG &mdash; Ragflow 1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=29a6c3e3"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="example:" href="rerank.html" />
    <link rel="prev" title="RAGFlow System Architecture" href="architecture.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Ragflow
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="about.html">About RAGFlow: Named Among GitHub’s Fastest-Growing Open Source Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="security-concerns.html">Security Concerns</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">RAGFlow System Architecture</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">From RAG to Context - A 2025 Year-End Review of RAG</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#can-rag-still-be-improved">Can RAG Still Be Improved?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-debate-about-long-context-and-rag">The Debate About Long Context and RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimizations-for-rag-conversational-quality">Optimizations for RAG Conversational Quality</a></li>
<li class="toctree-l3"><a class="reference internal" href="#from-knowledge-base-to-data-foundation">From Knowledge Base to Data Foundation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rerank.html">example:</a></li>
<li class="toctree-l1"><a class="reference internal" href="rerank.html#synergy-of-the-three-models">Synergy of the Three Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="vllm.html">Why vLLM is Used to Serve the Reranker Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="vllm-cpu.html">Serving vLLM Reranker Using Docker (CPU-Only)</a></li>
<li class="toctree-l1"><a class="reference internal" href="vllm-network.html">Integrating vLLM with RAGFlow via Docker Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Batch Processing and Metadata Management in Infiniflow RAGFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph.html">How the Knowledge Graph in Infiniflow/RAGFlow Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="llama-cpp.html">Running Llama 3.1 with llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiple-models-llamacpp.html">Running Multiple Models on llama.cpp Using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="hybrid.html">Deploying LLMs in Hybrid Cloud: Why llama.cpp Wins for Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="gvisor.html">How InfiniFlow RAGFlow Uses gVisor</a></li>
<li class="toctree-l1"><a class="reference internal" href="ragflow-gpu-cpu.html">RAGFlow GPU vs CPU: Full Explanation (2025 Edition)</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Upgrade to latest release :</a></li>
<li class="toctree-l1"><a class="reference internal" href="upload-document.html">upload document</a></li>
<li class="toctree-l1"><a class="reference internal" href="graphrag.html">Graphrag</a></li>
<li class="toctree-l1"><a class="reference internal" href="chat.html">Chat</a></li>
<li class="toctree-l1"><a class="reference internal" href="infinity.html">Why Infinity is a Good Alternative in RAGFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="minerU.html">MinerU and Its Use in RAGFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="agent-context-engine.html">What is Agent context engine?</a></li>
<li class="toctree-l1"><a class="reference internal" href="searxng.html">Using SearXNG with RAGFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="homelab.html">homelab</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Ragflow</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">From RAG to Context - A 2025 Year-End Review of RAG</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/blog-dec-2025.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="from-rag-to-context-a-2025-year-end-review-of-rag">
<h1>From RAG to Context - A 2025 Year-End Review of RAG<a class="headerlink" href="#from-rag-to-context-a-2025-year-end-review-of-rag" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://ragflow.io/blog">https://ragflow.io/blog</a>. On the ragflow website appears frequently a very insightful blog!</p>
<div class="line-block">
<div class="line"><strong>Date:</strong> December 22, 2025</div>
<div class="line"><strong>Source:</strong> RAGFlow Blog</div>
</div>
<p>As 2025 draws to a close, the field of Retrieval-Augmented Generation
(RAG) has undergone profound reflection, vigorous debate, and marked
evolution. Far from fading into obsolescence as some bold predictions
foresaw—amid lingering scepticism over its supposedly transient role—RAG
has solidified its indispensability as a cornerstone of data
infrastructure in the demanding arena of enterprise AI adoption.</p>
<p>Looking back, RAG’s trajectory this year has been complex. On one hand,
its practical effectiveness faced significant skepticism, partly due to
the “easy to use, hard to master” tuning challenges inherent to RAG
systems. On the other hand, its share of public attention seemed to be
overshadowed by the undisputed focus of 2025’s LLM applications: <strong>AI
Agents</strong>.</p>
<p>However, an intriguing trend emerged. Despite the controversies and not
being in the spotlight, enterprises genuinely committed to building core
AI competencies—especially mid-to-large-sized organizations—deepened and
systematized their investments in RAG. Rather than being marginalized,
RAG has solidified its core role in enterprise AI architecture. Its
position as critical infrastructure remains unshaken, forming the robust
foundation for enterprise intelligence.</p>
<p>Therefore, we must first move beyond surface-level debates to examine
the intrinsic vitality of RAG technology. Is it merely a transitional
“band-aid” to patch LLM knowledge gaps, or is it an architecture capable
of continuous evolution into a cornerstone for next-generation AI
applications? To answer this, we must systematically review its
technical improvements, architectural evolution, and new role in the age
of Agents.</p>
<section id="can-rag-still-be-improved">
<h2>Can RAG Still Be Improved?<a class="headerlink" href="#can-rag-still-be-improved" title="Link to this heading"></a></h2>
<section id="the-debate-about-long-context-and-rag">
<h3>The Debate About Long Context and RAG<a class="headerlink" href="#the-debate-about-long-context-and-rag" title="Link to this heading"></a></h3>
<p>In 2025, the core of many RAG debates stems from a widely acknowledged
contradiction: enterprises feel they “cannot live without RAG, yet
remain unsatisfied.” RAG lowers the barrier to accessing private
knowledge, but achieving stable and accurate results—especially for
complex queries—often requires extensive, fine-tuned optimization,
complicating total cost of ownership assessments.</p>
<p>Consequently, the theoretical question heatedly discussed in 2024—“Can
Long Context replace RAG?”—rapidly entered practical testing in 2025.
Some scenarios less sensitive to latency and cost, with relatively fixed
query patterns (e.g., certain contract reviews, fixed-format report
analysis), began experimenting with directly using long-context windows.
They feed entire or large batches of relevant documents into the model
at once, hoping to bypass potential information loss or noise from RAG
retrieval and directly address inconsistent conversational quality.</p>
<p>However, research since 2024 offers a clearer picture of the technical
comparison. Mechanically stuffing lengthy text into an LLM’s context
window is essentially a “brute-force” strategy. It inevitably scatters
the model’s attention, significantly degrading answer quality through
the <strong>“Lost in the Middle”</strong> or <strong>“information flooding”</strong> effect. More
importantly, this approach incurs high costs—computational overhead for
processing long context grows non-linearly.</p>
<p>Thus, for enterprises, the practical question is not engaging in
simplistic debates like “RAG is dead,” but returning to the core
challenge: how to incorporate the most relevant and effective
information into the model’s context processing system with the best
cost-performance ratio. This is precisely the original design goal of
RAG technology.</p>
<p>Improved long-context capabilities have not signaled RAG’s demise.
Instead, they prompt deeper thinking about how the two can collaborate.
For example, RAG systems can use long-context windows to hold more
complete, semantically coherent retrieved chunks or to aggregate
intermediate results for multi-step retrieval and reflection. This
“retrieval-first, long-context containment” synergy is a key driver
behind the emerging field of <strong>“Context Engineering.”</strong> It marks a shift
from optimizing single “retrieval algorithms” to the systematic design
of the end-to-end “retrieval-context assembly-model reasoning” pipeline.</p>
<p>Currently, paradigms for providing external knowledge to LLMs mainly
fall into four categories:</p>
<ol class="arabic simple">
<li><p>Relying solely on LLM’s long-context capability.</p></li>
<li><p>Utilizing KV Cache.</p></li>
<li><p>Using simple search methods like Grep.</p></li>
<li><p>Employing a full RAG architecture.</p></li>
</ol>
<p>Cost-wise, there is roughly a <strong>two-order-of-magnitude gap</strong> between
option 1 and option 4. Option 2 (KV Cache based) remains at least an
order of magnitude more expensive than full RAG, while facing serious
limitations in scalability, real-time updates, and complex enterprise
scenarios.</p>
<p>Option 3 (index-free / Grep-style) works in very narrow, highly
structured domains (e.g. clean codebases or logs) but fails completely
for the majority of enterprise unstructured/multi-modal data.</p>
</section>
<section id="optimizations-for-rag-conversational-quality">
<h3>Optimizations for RAG Conversational Quality<a class="headerlink" href="#optimizations-for-rag-conversational-quality" title="Link to this heading"></a></h3>
<p>A common source of inaccurate or unstable answers lies in a structural
conflict within the traditional “chunk-embed-retrieve” pipeline: using a
<strong>single-granularity, fixed-size</strong> text chunk to perform two inherently
conflicting tasks:</p>
<ul class="simple">
<li><p><strong>Semantic matching (recall)</strong>: Smaller chunks (100–256 tokens) →
better precision</p></li>
<li><p><strong>Context understanding (utilization)</strong>: Larger chunks (1024+ tokens)
→ better coherence</p></li>
</ul>
<p>This forces a difficult trade-off.</p>
<p>A fundamental improvement is to <strong>decouple</strong> the process into two
stages:</p>
<ul class="simple">
<li><p><strong>Search</strong> (scanning/locating): Use small, precise units for high
recall</p></li>
<li><p><strong>Retrieve</strong> (reading/understanding): Dynamically assemble larger,
coherent context</p></li>
</ul>
<figure class="align-default">
<img alt="_images/tree-rag.png" src="_images/tree-rag.png" />
</figure>
<p><strong>RAGFlow’s TreeRAG</strong> technology embodies this:</p>
<ul class="simple">
<li><p><strong>Offline</strong>: LLM builds hierarchical tree summaries (Chapter →
Section → Subsection → Key Paragraph)</p></li>
<li><p><strong>Online</strong>: Precise small-chunk search → use tree as navigation map →
auto-expand to complete logical fragments</p></li>
</ul>
<p>This mitigates <strong>“Lost in the Middle”</strong> and context fragmentation.</p>
<p>For even more complex queries (scattered info, cross-document reasoning)
→ <strong>GraphRAG</strong> (entity-relationship graphs) becomes relevant, though it
has challenges:</p>
<ul class="simple">
<li><p>Massive token consumption during graph building</p></li>
<li><p>Noisy auto-extracted entities/relations</p></li>
<li><p>Fragmented knowledge output</p></li>
</ul>
<p><strong>Hybrid TreeRAG + GraphRAG</strong> (“Long-Context RAG”) approaches appear
most promising.</p>
<p>Modern RAG philosophy: Leverage LLMs during <strong>ingestion</strong> for deep
semantic enhancement (summaries, entities, metadata, potential
questions) → use this as intelligent “navigation map” during
<strong>retrieval</strong> → achieve optimal balance of effectiveness, performance
and cost.</p>
</section>
<section id="from-knowledge-base-to-data-foundation">
<h3>From Knowledge Base to Data Foundation<a class="headerlink" href="#from-knowledge-base-to-data-foundation" title="Link to this heading"></a></h3>
<p>RAG is an architectural paradigm, not just a Q&amp;A tool.</p>
<p>With the rise of <strong>AI Agents</strong>, enterprise RAG is evolving into a
<strong>general-purpose data foundation</strong> for unstructured data — serving as
unified, efficient, secure access layer for all types of Agents.</p>
<p>A robust, scalable, configurable <strong>Ingestion Pipeline</strong> has become the
core of modern RAG engines, handling the full lifecycle from raw
documents to structured, semantically rich, query-ready knowledge.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="architecture.html" class="btn btn-neutral float-left" title="RAGFlow System Architecture" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="rerank.html" class="btn btn-neutral float-right" title="example:" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Jansen Jan.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>